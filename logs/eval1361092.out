Using the latest cached version of the dataset since simonjegou/loogle couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'shortdep_qa' at /lustre/fsn1/projects/rech/awr/uof65ov/hf_cache/simonjegou___loogle/shortdep_qa/0.0.0/6e3eaf28d24ce34933170c85de36be2448d0d2a7 (last modified on Mon Nov 25 15:20:25 2024).
Device set to use cuda:0
  0%|          | 0/86 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
  1%|          | 1/86 [00:03<04:30,  3.18s/it]  2%|▏         | 2/86 [00:05<03:37,  2.59s/it]  3%|▎         | 3/86 [00:11<05:46,  4.17s/it]  5%|▍         | 4/86 [00:13<04:28,  3.28s/it]  6%|▌         | 5/86 [00:13<03:04,  2.28s/it]  7%|▋         | 6/86 [00:15<02:38,  1.98s/it]  8%|▊         | 7/86 [00:19<03:45,  2.86s/it]  9%|▉         | 8/86 [00:27<05:50,  4.49s/it] 10%|█         | 9/86 [00:28<04:08,  3.22s/it] 12%|█▏        | 10/86 [00:28<03:00,  2.37s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
 13%|█▎        | 11/86 [00:30<02:31,  2.02s/it] 14%|█▍        | 12/86 [00:31<02:12,  1.79s/it] 15%|█▌        | 13/86 [00:32<01:58,  1.62s/it] 16%|█▋        | 14/86 [00:43<05:28,  4.56s/it] 17%|█▋        | 15/86 [00:51<06:37,  5.59s/it] 19%|█▊        | 16/86 [00:52<04:48,  4.12s/it] 20%|█▉        | 17/86 [00:54<03:55,  3.41s/it] 21%|██        | 18/86 [00:59<04:37,  4.08s/it] 22%|██▏       | 19/86 [01:04<04:40,  4.19s/it] 23%|██▎       | 20/86 [01:05<03:28,  3.16s/it] 24%|██▍       | 21/86 [01:12<04:38,  4.29s/it] 26%|██▌       | 22/86 [01:12<03:29,  3.28s/it] 27%|██▋       | 23/86 [01:29<07:39,  7.30s/it] 28%|██▊       | 24/86 [01:34<06:43,  6.51s/it] 29%|██▉       | 25/86 [01:38<05:54,  5.82s/it] 30%|███       | 26/86 [01:40<04:39,  4.65s/it] 31%|███▏      | 27/86 [01:40<03:18,  3.36s/it] 33%|███▎      | 28/86 [01:42<02:44,  2.83s/it] 34%|███▎      | 29/86 [01:43<02:03,  2.16s/it] 35%|███▍      | 30/86 [01:43<01:40,  1.79s/it] 36%|███▌      | 31/86 [01:45<01:26,  1.58s/it] 37%|███▋      | 32/86 [01:46<01:24,  1.56s/it] 38%|███▊      | 33/86 [01:50<02:00,  2.28s/it] 40%|███▉      | 34/86 [01:51<01:36,  1.85s/it] 41%|████      | 35/86 [01:53<01:46,  2.08s/it] 42%|████▏     | 36/86 [01:54<01:25,  1.71s/it] 43%|████▎     | 37/86 [02:02<02:53,  3.54s/it] 44%|████▍     | 38/86 [02:04<02:24,  3.01s/it] 45%|████▌     | 39/86 [02:04<01:43,  2.21s/it] 47%|████▋     | 40/86 [02:08<02:01,  2.63s/it] 48%|████▊     | 41/86 [02:12<02:18,  3.07s/it] 49%|████▉     | 42/86 [02:13<01:44,  2.38s/it] 50%|█████     | 43/86 [02:16<01:50,  2.56s/it] 51%|█████     | 44/86 [02:17<01:31,  2.18s/it] 52%|█████▏    | 45/86 [02:19<01:28,  2.15s/it] 53%|█████▎    | 46/86 [02:20<01:12,  1.82s/it] 55%|█████▍    | 47/86 [02:22<01:09,  1.79s/it] 56%|█████▌    | 48/86 [02:23<00:59,  1.56s/it] 57%|█████▋    | 49/86 [02:25<01:01,  1.66s/it] 58%|█████▊    | 50/86 [02:26<00:54,  1.52s/it] 59%|█████▉    | 51/86 [02:27<00:44,  1.26s/it] 60%|██████    | 52/86 [02:28<00:46,  1.36s/it] 62%|██████▏   | 53/86 [02:30<00:50,  1.52s/it] 63%|██████▎   | 54/86 [02:31<00:42,  1.31s/it] 64%|██████▍   | 55/86 [02:32<00:37,  1.21s/it] 65%|██████▌   | 56/86 [02:40<01:35,  3.17s/it] 66%|██████▋   | 57/86 [02:40<01:09,  2.38s/it] 67%|██████▋   | 58/86 [02:41<00:54,  1.96s/it] 69%|██████▊   | 59/86 [02:47<01:25,  3.15s/it] 70%|██████▉   | 60/86 [02:49<01:12,  2.79s/it] 71%|███████   | 61/86 [02:50<00:57,  2.29s/it] 72%|███████▏  | 62/86 [02:51<00:42,  1.78s/it] 73%|███████▎  | 63/86 [02:52<00:35,  1.54s/it] 74%|███████▍  | 64/86 [02:52<00:28,  1.29s/it] 76%|███████▌  | 65/86 [02:53<00:23,  1.14s/it] 77%|███████▋  | 66/86 [02:54<00:18,  1.07it/s] 78%|███████▊  | 67/86 [02:55<00:19,  1.05s/it] 79%|███████▉  | 68/86 [02:56<00:16,  1.06it/s] 80%|████████  | 69/86 [02:56<00:15,  1.12it/s] 81%|████████▏ | 70/86 [02:57<00:12,  1.32it/s] 83%|████████▎ | 71/86 [03:08<00:57,  3.86s/it] 84%|████████▎ | 72/86 [03:08<00:39,  2.83s/it] 85%|████████▍ | 73/86 [03:09<00:28,  2.19s/it] 86%|████████▌ | 74/86 [03:12<00:27,  2.26s/it] 87%|████████▋ | 75/86 [03:12<00:20,  1.83s/it] 88%|████████▊ | 76/86 [03:13<00:14,  1.49s/it] 90%|████████▉ | 77/86 [03:14<00:11,  1.31s/it] 91%|█████████ | 78/86 [03:16<00:13,  1.65s/it] 92%|█████████▏| 79/86 [03:18<00:10,  1.54s/it] 93%|█████████▎| 80/86 [03:18<00:07,  1.27s/it] 94%|█████████▍| 81/86 [03:19<00:05,  1.08s/it] 95%|█████████▌| 82/86 [03:20<00:04,  1.04s/it] 97%|█████████▋| 83/86 [03:21<00:03,  1.06s/it] 98%|█████████▊| 84/86 [03:25<00:03,  1.88s/it] 99%|█████████▉| 85/86 [03:26<00:01,  1.69s/it]100%|██████████| 86/86 [03:26<00:00,  1.32s/it]100%|██████████| 86/86 [03:26<00:00,  2.41s/it]
[nltk_data] Error loading wordnet: <urlopen error [Errno 101] Network
[nltk_data]     is unreachable>
/lustre/fswork/projects/rech/awr/uof65ov/kv_cache_comp/venv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/lustre/fswork/projects/rech/awr/uof65ov/kv_cache_comp/venv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/lustre/fswork/projects/rech/awr/uof65ov/kv_cache_comp/venv/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'shortdep_qa': {'bleu1': 0.10551438555214745, 'bleu4': 0.019221391878060008, 'rouge-1': 0.29009090384060615, 'rouge-2': 0.12369516664010645, 'rouge-l': 0.27385266245232903, 'meteor': 0.13618292589586437, 'bert': 0.8493672013282776}}
