Using the latest cached version of the dataset since MaxJeblick/InfiniteBench couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'longbook_qa_eng' at /lustre/fsn1/projects/rech/awr/uof65ov/hf_cache/MaxJeblick___infinite_bench/longbook_qa_eng/0.0.0/aaead86e917560307fb0fd5b5c53da52f663865f (last modified on Tue Nov 26 23:57:29 2024).
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.70it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00, 10.50it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 10.81it/s]
  0%|          | 0/69 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
  1%|▏         | 1/69 [00:18<21:07, 18.64s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (407183 > 131072). Running this sequence through the model will result in indexing errors
Context length has been truncated from 407183 to 131072 tokens.
  3%|▎         | 2/69 [00:55<32:30, 29.11s/it]Context length has been truncated from 155604 to 131072 tokens.
  4%|▍         | 3/69 [01:33<36:37, 33.30s/it]  6%|▌         | 4/69 [01:54<31:04, 28.68s/it]  7%|▋         | 5/69 [02:14<27:06, 25.41s/it]Context length has been truncated from 142851 to 131072 tokens.
  9%|▊         | 6/69 [02:50<30:32, 29.09s/it] 10%|█         | 7/69 [03:10<26:50, 25.97s/it] 12%|█▏        | 8/69 [03:34<25:55, 25.49s/it]Context length has been truncated from 189422 to 131072 tokens.
 13%|█▎        | 9/69 [04:08<28:01, 28.02s/it]Context length has been truncated from 223465 to 131072 tokens.
 14%|█▍        | 10/69 [04:43<29:33, 30.05s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Context length has been truncated from 232373 to 131072 tokens.
 16%|█▌        | 11/69 [05:20<31:17, 32.37s/it]Context length has been truncated from 266590 to 131072 tokens.
 17%|█▋        | 12/69 [05:55<31:35, 33.26s/it]Context length has been truncated from 201523 to 131072 tokens.
 19%|█▉        | 13/69 [06:32<32:02, 34.33s/it]Context length has been truncated from 134060 to 131072 tokens.
 20%|██        | 14/69 [07:06<31:12, 34.05s/it]Context length has been truncated from 183236 to 131072 tokens.
 22%|██▏       | 15/69 [07:40<30:42, 34.11s/it]Context length has been truncated from 398587 to 131072 tokens.
 23%|██▎       | 16/69 [08:17<30:49, 34.90s/it] 25%|██▍       | 17/69 [08:36<26:08, 30.16s/it]Context length has been truncated from 199356 to 131072 tokens.
 26%|██▌       | 18/69 [09:13<27:27, 32.31s/it]Context length has been truncated from 213785 to 131072 tokens.
 28%|██▊       | 19/69 [09:49<27:50, 33.41s/it] 29%|██▉       | 20/69 [10:04<22:45, 27.88s/it]Context length has been truncated from 282547 to 131072 tokens.
 30%|███       | 21/69 [10:39<23:59, 30.00s/it]Context length has been truncated from 148091 to 131072 tokens.
 32%|███▏      | 22/69 [11:16<25:11, 32.15s/it]Context length has been truncated from 194698 to 131072 tokens.
 33%|███▎      | 23/69 [11:53<25:44, 33.58s/it] 35%|███▍      | 24/69 [12:30<25:54, 34.55s/it]Context length has been truncated from 237794 to 131072 tokens.
 36%|███▌      | 25/69 [13:08<26:07, 35.62s/it]Context length has been truncated from 301178 to 131072 tokens.
 38%|███▊      | 26/69 [13:43<25:24, 35.44s/it]Context length has been truncated from 180445 to 131072 tokens.
 39%|███▉      | 27/69 [14:18<24:42, 35.30s/it]Context length has been truncated from 261046 to 131072 tokens.
 41%|████      | 28/69 [14:54<24:19, 35.61s/it] 42%|████▏     | 29/69 [15:15<20:40, 31.01s/it] 43%|████▎     | 30/69 [15:35<18:03, 27.78s/it]Context length has been truncated from 213441 to 131072 tokens.
 45%|████▍     | 31/69 [16:09<18:44, 29.59s/it]Context length has been truncated from 188157 to 131072 tokens.
 46%|████▋     | 32/69 [16:45<19:31, 31.66s/it] 48%|████▊     | 33/69 [17:16<18:52, 31.46s/it] 49%|████▉     | 34/69 [17:38<16:44, 28.69s/it]Context length has been truncated from 244874 to 131072 tokens.
 51%|█████     | 35/69 [18:16<17:45, 31.34s/it] 52%|█████▏    | 36/69 [18:35<15:11, 27.61s/it]Context length has been truncated from 177477 to 131072 tokens.
 54%|█████▎    | 37/69 [19:10<15:54, 29.81s/it]Context length has been truncated from 145802 to 131072 tokens.
 55%|█████▌    | 38/69 [19:44<16:03, 31.07s/it] 57%|█████▋    | 39/69 [20:17<15:55, 31.84s/it]Context length has been truncated from 163637 to 131072 tokens.
 58%|█████▊    | 40/69 [20:51<15:40, 32.44s/it] 59%|█████▉    | 41/69 [21:07<12:46, 27.38s/it] 61%|██████    | 42/69 [21:25<11:02, 24.52s/it]Context length has been truncated from 279569 to 131072 tokens.
 62%|██████▏   | 43/69 [21:59<11:51, 27.38s/it]Context length has been truncated from 264026 to 131072 tokens.
 64%|██████▍   | 44/69 [22:35<12:31, 30.05s/it] 65%|██████▌   | 45/69 [23:05<12:02, 30.09s/it]Context length has been truncated from 144034 to 131072 tokens.
 67%|██████▋   | 46/69 [23:40<12:07, 31.62s/it]Context length has been truncated from 148556 to 131072 tokens.
 68%|██████▊   | 47/69 [24:15<11:57, 32.62s/it]Context length has been truncated from 178119 to 131072 tokens.
 70%|██████▉   | 48/69 [24:51<11:41, 33.42s/it] 71%|███████   | 49/69 [25:10<09:45, 29.27s/it]Context length has been truncated from 604752 to 131072 tokens.
 72%|███████▏  | 50/69 [25:46<09:56, 31.39s/it]Context length has been truncated from 204237 to 131072 tokens.
 74%|███████▍  | 51/69 [26:22<09:45, 32.51s/it] 75%|███████▌  | 52/69 [26:39<07:56, 28.05s/it] 77%|███████▋  | 53/69 [26:59<06:48, 25.52s/it]Context length has been truncated from 233473 to 131072 tokens.
 78%|███████▊  | 54/69 [27:35<07:10, 28.72s/it]Context length has been truncated from 255476 to 131072 tokens.
 80%|███████▉  | 55/69 [28:13<07:22, 31.60s/it] 81%|████████  | 56/69 [28:33<06:03, 27.98s/it]Context length has been truncated from 222834 to 131072 tokens.
 83%|████████▎ | 57/69 [29:07<05:57, 29.77s/it] 84%|████████▍ | 58/69 [29:38<05:31, 30.13s/it]Context length has been truncated from 452633 to 131072 tokens.
 86%|████████▌ | 59/69 [30:14<05:20, 32.04s/it]Context length has been truncated from 152805 to 131072 tokens.
 87%|████████▋ | 60/69 [30:53<05:05, 33.96s/it]Context length has been truncated from 135020 to 131072 tokens.
 88%|████████▊ | 61/69 [31:28<04:35, 34.46s/it]Context length has been truncated from 155800 to 131072 tokens.
 90%|████████▉ | 62/69 [32:02<03:59, 34.14s/it]Context length has been truncated from 539229 to 131072 tokens.
 91%|█████████▏| 63/69 [32:36<03:25, 34.21s/it]Context length has been truncated from 384805 to 131072 tokens.
 93%|█████████▎| 64/69 [33:11<02:52, 34.44s/it]Context length has been truncated from 265568 to 131072 tokens.
 94%|█████████▍| 65/69 [33:45<02:17, 34.37s/it]Context length has been truncated from 132858 to 131072 tokens.
 96%|█████████▌| 66/69 [34:18<01:41, 33.91s/it]Context length has been truncated from 744467 to 131072 tokens.
 97%|█████████▋| 67/69 [34:56<01:10, 35.19s/it]Context length has been truncated from 245768 to 131072 tokens.
 99%|█████████▊| 68/69 [35:32<00:35, 35.30s/it]100%|██████████| 69/69 [35:49<00:00, 29.96s/it]100%|██████████| 69/69 [35:49<00:00, 31.16s/it]
0it [00:00, ?it/s]351it [00:00, 39027.64it/s]
0.028982198097927014
